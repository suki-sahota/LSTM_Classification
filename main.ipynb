{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training, dev and unlabeled test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following provides a starting code (Python 3) of how to read the labeled training and dev cipher text, and unlabeled test cipher text, into lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, dev, test = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16220\n",
      "[[0, 'lkêcê yoúc cêêö y#êjl lw mówám Újám j Úêê# ütlk Úol lkêú z#ê ctöé8ú ówl xoóóú éê#xw#öê#c .'], [0, '6êcétlê jolêot8 zc éê#xw#öjóáê , tl zc j #jlkê# 8tcl8êcc jöÚ8ê 6wüó lkê öt668ê wx lkê #wj6 , ükê#ê lkê lkêöjltá t#wótêc j#ê lww wÚ2twoc jó6 lkê cê+oj8 éw8tltác lww cöoy .'], [0, 'tx lktc kw8t6jú öw2tê tc coééwcê6 lw Úê j ytxl , cwöêÚw6ú oóü#jééê6 tl êj#8ú , lwwm wol j88 lkê yww6 cloxx , jó6 8êxl Úêktó6 lkê á#jé ( 8tlê#j88ú ) .']]\n"
     ]
    }
   ],
   "source": [
    "for x in open('./train_enc.tsv', encoding='utf-8'):\n",
    "    x = x.rstrip('\\n\\r').split('\\t')\n",
    "    # x[0] will be the label (0 or 1), and x[1] will be the ciphertext sentence.\n",
    "    x[0] = int(x[0]) \n",
    "    train.append(x)\n",
    "print (len(train))\n",
    "print (train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2027\n",
      "[[1, 'ów8jó Ú#j2ê8ú l#êj6c ükê#ê xêü jöê#tájó xt8öc 6j#ê lw 6ê82ê 77 tólw lkê üw#86 wx jöÚt2j8êóáê jó6 jöÚtyotlú <<<'], [0, 'ê2êó öo#ékú zc ê+éê#l áwötá ltötóy jó6 xjöê6 ákj#tcöj áj ózl #êcáoê lktc êxxw#l .'], [1, 'üt88 jcco#ê68ú #jóm jc wóê wx lkê á8ê2ê#êcl , öwcl 6êáêélt2ê8ú jöoctóy áwöê6têc wx lkê úêj# .']]\n"
     ]
    }
   ],
   "source": [
    "for x in open('./dev_enc.tsv', encoding='utf-8'):\n",
    "    x = x.rstrip('\\n\\r').split('\\t')\n",
    "    # x[0] will be the label (0 or 1), and x[1] will be the ciphertext sentence.\n",
    "    x[0] = int(x[0]) \n",
    "    dev.append(x)\n",
    "print (len(dev))\n",
    "print (dev[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different from 'train' and 'dev' that are both list of tuples, 'test' will be just a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2028\n",
      "['j 6t6jáltá jó6 6o88 6wáoöêólj#ú y8w#txútóy cwxlüj#ê jój#ákú .', 'ówlktóy cltámc , #êj88ú , ê+áêél j 8tóyê#tóy á#êêétóêcc wóê xêê8c x#wö Úêtóy 6#jyyê6 lk#woyk j cj6 , cw#6t6 oót2ê#cê wx yoóc , 6#oyc , j2j#táê jó6 6jöjyê6 6#êjöc .', 'öo#ékú jó6 üt8cwó jáloj88ú öjmê j é#êllú yww6 lêjö <<< Úol lkê é#wvêál co##woó6tóy lkêö tc 6tcl#êcctóy8ú #wlê .']\n"
     ]
    }
   ],
   "source": [
    "for x in open('./test_enc_unlabeled.tsv', encoding='utf-8'):\n",
    "    x = x.rstrip('\\n\\r')\n",
    "    test.append(x)\n",
    "print (len(test))\n",
    "print (test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can split every sentence into lists of words by white spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = [[x[0], x[1].split(' ')] for x in train]\n",
    "dev_split = [[x[0], x[1].split(' ')] for x in dev]\n",
    "test_split = [[x.split(' ')] for x in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code Body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may choose to experiment with different methods using your program. However, you need to embed the training and inference processes at here. We will use your prediction on the unlabeled test data to grade, while checking this part to understand how your method has produced the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eventually, results need to be a list of 2028 0 or 1's\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from itertools import islice\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/Wiki-words-250-with-normalization/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_encoded_ciphers(encoded_ciphers, my_max_length):\n",
    "    \n",
    "    my_shorter_length = (9/10) * my_max_length\n",
    "    my_shorter_length = int(my_shorter_length)\n",
    "    \n",
    "    padded_ciphers_encoding = []\n",
    "    \n",
    "    for enc_cipher in encoded_ciphers:\n",
    "        \n",
    "        zero_padding_cnt = my_max_length - enc_cipher.shape[0]\n",
    "        #zero_padding_cnt = my_shorter_length - enc_cipher.shape[0]\n",
    "        \n",
    "        if zero_padding_cnt > 0:\n",
    "            #Pad array list by appropriate amount\n",
    "            pad = np.zeros((1, 250))\n",
    "            for i in range(zero_padding_cnt):\n",
    "                enc_cipher = np.concatenate((pad, enc_cipher), axis=0)\n",
    "        else:\n",
    "            #Trim the array list down by appropriate amount\n",
    "            cipher_length = len(enc_cipher)\n",
    "            enc_cipher = enc_cipher[:cipher_length+zero_padding_cnt]\n",
    "            \n",
    "        padded_ciphers_encoding.append(enc_cipher)\n",
    "    return padded_ciphers_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find max number of columns\n",
    "X_train = [x[1] for x in train_split]\n",
    "m = len(X_train)\n",
    "n = len(max(X_train, key=len)) #Max number of columns in training data\n",
    "\n",
    "lengths = [len(x) for x in X_train]\n",
    "med = int(statistics.median(lengths))\n",
    "\n",
    "x_val = [x[1] for x in dev_split]\n",
    "m_prime = len(x_val)\n",
    "n_prime = len(max(x_val, key=len)) #Max number of columns in development data\n",
    "\n",
    "x_test = [x[0] for x in test_split]\n",
    "m_double_prime = len(x_test)\n",
    "n_double_prime = len(max(x_test, key=len)) #Max number of columns in test data\n",
    "\n",
    "max_length = max(n, n_prime, n_double_prime)\n",
    "\n",
    "max_length = int(med)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "###TRAINING DATA###\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_plain = [x[1] for x in train_split]\n",
    "\n",
    "#Remove stop words from Training Data\n",
    "countOfWord = {} #Dictionaries for corpus\n",
    "\n",
    "#Adds each unseen word into countOfWord vocabulary\n",
    "for row in X_train_plain:\n",
    "    for idx, word in enumerate(row):\n",
    "        \n",
    "        if word in countOfWord:\n",
    "            #Increment the count for that word in vocab\n",
    "            countOfWord[word] += 1\n",
    "        else:\n",
    "            countOfWord[word] = 1\n",
    "            \n",
    "    #Pad the end of this row with EOS\n",
    "    #length = len(row)\n",
    "    #np.pad(row, (0, max_length - length), 'constant', constant_values=\"\\s\")\n",
    "            \n",
    "#Sort countOfWord vocabulary by frequency\n",
    "countOfWord = dict(sorted(countOfWord.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "k = 8\n",
    "#Remove top k frequent words (possibly stop words)\n",
    "for itr in range(k):\n",
    "    countOfWord.popitem()\n",
    "    \n",
    "#Translate each stop word into an EOS\n",
    "for row in X_train_plain:\n",
    "    for idx, word in enumerate(row):\n",
    "        #Replace stop words with EOS\n",
    "        if word not in countOfWord:\n",
    "            row[idx] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data X\n",
    "X_train = [embed(x) for x in X_train_plain]\n",
    "X_train = get_padded_encoded_ciphers(X_train, max_length)\n",
    "\n",
    "X_train = tf.convert_to_tensor(X_train)\n",
    "\n",
    "###################\n",
    "#Training data Y\n",
    "Y_train = [float(y[0]) for y in train_split]\n",
    "\n",
    "Y_train = tf.convert_to_tensor(Y_train)\n",
    "Y_train = tf.cast(Y_train, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "##DEVELOPMENT DATA#\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_plain = [x[1] for x in dev_split]\n",
    "\n",
    "#Remove stop words from Development Data\n",
    "countOfWordDev = {} #Dictionaries for corpus\n",
    "\n",
    "#Adds each unseen word into countOfWord vocabulary\n",
    "for row in x_val_plain:\n",
    "    for idx, word in enumerate(row):\n",
    "        \n",
    "        if word in countOfWordDev:\n",
    "            #Increment the count for that word in vocab\n",
    "            countOfWordDev[word] += 1\n",
    "        else:\n",
    "            countOfWordDev[word] = 1\n",
    "            \n",
    "    #Pad the end of this row with EOS\n",
    "    #length = len(row)\n",
    "    #np.pad(row, (0, max_length - length), 'constant', constant_values=0)\n",
    "            \n",
    "#Sort countOfWordDev vocabulary by frequency\n",
    "countOfWordDev = dict(sorted(countOfWordDev.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "k = 8\n",
    "#Remove top k frequent words (possibly stop words)\n",
    "for itr in range(k):\n",
    "    countOfWordDev.popitem()\n",
    "    \n",
    "#Translate each stop word into an EOS\n",
    "for row in x_val_plain:\n",
    "    for idx, word in enumerate(row):\n",
    "        #Replace stop words with EOS\n",
    "        if word not in countOfWordDev:\n",
    "            row[idx] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Development data X\n",
    "x_val = [embed(x) for x in x_val_plain]\n",
    "x_val = get_padded_encoded_ciphers(x_val, max_length)\n",
    "\n",
    "x_val = tf.convert_to_tensor(x_val)\n",
    "\n",
    "###################\n",
    "#Development data Y\n",
    "y_val = [x[0] for x in dev_split]\n",
    "\n",
    "y_val = tf.convert_to_tensor(y_val)\n",
    "y_val = tf.cast(y_val, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "#####TEST DATA#####\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_plain = [x[0] for x in test_split]\n",
    "\n",
    "#Remove stop words from Test Data\n",
    "countOfWordTest = {} #Dictionaries for corpus\n",
    "\n",
    "#Adds each unseen word into countOfWord vocabulary\n",
    "for row in x_test_plain:\n",
    "    for idx, word in enumerate(row):\n",
    "        \n",
    "        if word in countOfWordTest:\n",
    "            #Increment the count for that word in vocab\n",
    "            countOfWordTest[word] += 1\n",
    "        else:\n",
    "            countOfWordTest[word] = 1\n",
    "            \n",
    "    #Pad the end of this row with EOS\n",
    "    #length = len(row)\n",
    "    #np.pad(row, (0, max_length - length), 'constant', constant_values=0)\n",
    "            \n",
    "#Sort countOfWordTest vocabulary by frequency\n",
    "countOfWordTest = dict(sorted(countOfWordTest.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "k = 8\n",
    "#Remove top k frequent words (possibly stop words)\n",
    "for itr in range(k):\n",
    "    countOfWordTest.popitem()\n",
    "    \n",
    "#Translate each stop word into an EOS\n",
    "for row in x_test_plain:\n",
    "    for idx, word in enumerate(row):\n",
    "        #Replace stop words with EOS\n",
    "        if word not in countOfWordTest:\n",
    "            row[idx] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test data X\n",
    "x_test = [embed(x) for x in x_test_plain]\n",
    "x_test = get_padded_encoded_ciphers(x_test, max_length)\n",
    "\n",
    "x_test = tf.convert_to_tensor(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "#######MODEL#######\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=128, activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=8))\n",
    "\n",
    "model.add(LSTM(units=128, activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=8))\n",
    "\n",
    "model.add(LSTM(units=128, activation='tanh', return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=8))\n",
    "\n",
    "model.add(LSTM(units=128, activation='tanh', return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0025), \n",
    "    metrics='accuracy'\n",
    ")\n",
    "callback = EarlyStopping(monitor='loss', patience=2, mode='min')     #Training\n",
    "callback_two = EarlyStopping(monitor='loss', patience=2, mode='min') #Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "601/601 [==============================] - 54s 90ms/step - loss: 0.3212 - accuracy: 0.8625 - val_loss: 0.2730 - val_accuracy: 0.8860\n",
      "Epoch 2/100\n",
      "601/601 [==============================] - 55s 91ms/step - loss: 0.2625 - accuracy: 0.8893 - val_loss: 0.3033 - val_accuracy: 0.8683\n",
      "Epoch 3/100\n",
      "601/601 [==============================] - 54s 90ms/step - loss: 0.2329 - accuracy: 0.8992 - val_loss: 0.3234 - val_accuracy: 0.8584\n",
      "Epoch 4/100\n",
      "601/601 [==============================] - 54s 90ms/step - loss: 0.2295 - accuracy: 0.9006 - val_loss: 0.3287 - val_accuracy: 0.8737\n",
      "Epoch 5/100\n",
      "601/601 [==============================] - 57s 94ms/step - loss: 0.2055 - accuracy: 0.9089 - val_loss: 0.3720 - val_accuracy: 0.8510\n",
      "Epoch 6/100\n",
      "601/601 [==============================] - 55s 91ms/step - loss: 0.2345 - accuracy: 0.9000 - val_loss: 0.4136 - val_accuracy: 0.8367\n",
      "Epoch 7/100\n",
      "601/601 [==============================] - 55s 91ms/step - loss: 0.2080 - accuracy: 0.9099 - val_loss: 0.4125 - val_accuracy: 0.8500\n"
     ]
    }
   ],
   "source": [
    "#Train model\n",
    "history = model.fit(\n",
    "    X_train, Y_train, batch_size=27, epochs=100, \n",
    "    callbacks=callback, validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "62/62 [==============================] - 5s 83ms/step - loss: 0.2305 - accuracy: 0.9087\n",
      "Epoch 2/100\n",
      "62/62 [==============================] - 5s 85ms/step - loss: 0.1575 - accuracy: 0.9373\n",
      "Epoch 3/100\n",
      "62/62 [==============================] - 5s 87ms/step - loss: 0.1648 - accuracy: 0.9285\n",
      "Epoch 4/100\n",
      "62/62 [==============================] - 6s 89ms/step - loss: 0.1920 - accuracy: 0.9245\n"
     ]
    }
   ],
   "source": [
    "#Train model on validation data as well\n",
    "history_two = model.fit(\n",
    "    x_val, y_val, batch_size=33, \n",
    "    epochs=100, callbacks=callback_two\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_85_layer_call_fn, lstm_cell_85_layer_call_and_return_conditional_losses, lstm_cell_86_layer_call_fn, lstm_cell_86_layer_call_and_return_conditional_losses, lstm_cell_87_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/seven_layer_eighty_seven_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/seven_layer_eighty_seven_model/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x10e354100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x10e3572b0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x10e36e520> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x10e34c370> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "#Save Model\n",
    "#my_model_path = 'saved_models/seven_layer_eighty_eight_model'\n",
    "#model.save(my_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_83 (LSTM)              (None, 20, 128)           194048    \n",
      "                                                                 \n",
      " dropout_83 (Dropout)        (None, 20, 128)           0         \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 20, 8)             1032      \n",
      "                                                                 \n",
      " lstm_84 (LSTM)              (None, 20, 128)           70144     \n",
      "                                                                 \n",
      " dropout_84 (Dropout)        (None, 20, 128)           0         \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 20, 8)             1032      \n",
      "                                                                 \n",
      " lstm_85 (LSTM)              (None, 20, 128)           70144     \n",
      "                                                                 \n",
      " dropout_85 (Dropout)        (None, 20, 128)           0         \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 20, 8)             1032      \n",
      "                                                                 \n",
      " lstm_86 (LSTM)              (None, 128)               70144     \n",
      "                                                                 \n",
      " dropout_86 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 407,705\n",
      "Trainable params: 407,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Load Model\n",
    "#model = tf.keras.models.load_model('saved_models/seven_layer_eighty_six_model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 31ms/step - loss: 0.3303 - accuracy: 0.8747\n",
      "loss:  0.3303125500679016\n",
      "accuracy:  0.8746916651725769\n"
     ]
    }
   ],
   "source": [
    "#Evaluate on development data\n",
    "loss, accuracy = model.evaluate(x_val, y_val)\n",
    "print('loss: ', loss)\n",
    "print('accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict on test data\n",
    "y_hat = model.predict(x_test)\n",
    "y_hat_modified = [0 if val <0.5 else 1 for val in y_hat]\n",
    "results = y_hat_modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Prediction Result File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to submit a prediction result file. It should have 2028 lines, every line should be either 0 or 1, which is your model's prediction on the respective test set instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose you had your model's predictions on the 2028 test cases read from test_enc_unlabeled.tsv, and \n",
    "#those results are in the list called 'results'\n",
    "assert (len(results) == 2028)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the results are not float numbers, but intergers 0 and 1\n",
    "results = [int(x) for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your prediction results to 'upload_predictions.txt' and upload that later\n",
    "with open('upload_predictions.txt', 'w', encoding = 'utf-8') as fp:\n",
    "    for x in results:\n",
    "        fp.write(str(x) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
